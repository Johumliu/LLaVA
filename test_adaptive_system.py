#!/usr/bin/env python3
"""
æµ‹è¯•è‡ªé€‚åº”å±‚é€‰æ‹©å’ŒMOEæŠ•å½±å™¨ç³»ç»Ÿ
"""

import torch
import torch.nn as nn
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'llava'))

from llava.model.multimodal_encoder.adaptive_clip_encoder import AdaptiveCLIPVisionTower, LayerClassifier
from llava.model.multimodal_projector.moe_projector import MoEProjector, AdaptiveMoEProjector


def test_layer_classifier():
    """æµ‹è¯•å±‚åˆ†ç±»å™¨"""
    print("=== æµ‹è¯•å±‚åˆ†ç±»å™¨ ===")
    
    # åˆ›å»ºæ¨¡æ‹Ÿå‚æ•°
    class MockArgs:
        def __init__(self):
            self.mm_vision_select_layer = -2
            self.mm_vision_select_feature = 'patch'
            self.use_adaptive_layer_selection = True
            self.top_k_layers = 3
            self.training_mode = True
    
    args = MockArgs()
    
    # åˆ›å»ºå±‚åˆ†ç±»å™¨
    vision_hidden_size = 1024
    num_layers = 24
    classifier = LayerClassifier(vision_hidden_size, num_layers)
    
    # åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥
    batch_size = 2
    seq_len = 196  # 14x14 patches
    hidden_size = vision_hidden_size
    
    image_features = torch.randn(batch_size, seq_len, hidden_size)
    text_features = torch.randn(batch_size, seq_len, hidden_size)
    
    # å‰å‘ä¼ æ’­
    layer_probs, layer_logits = classifier(image_features, text_features)
    
    print(f"è¾“å…¥å½¢çŠ¶: {image_features.shape}")
    print(f"è¾“å‡ºæ¦‚ç‡å½¢çŠ¶: {layer_probs.shape}")
    print(f"è¾“å‡ºlogitså½¢çŠ¶: {layer_logits.shape}")
    print(f"æ¦‚ç‡åˆ†å¸ƒ: {layer_probs[0]}")
    print(f"é€‰æ‹©çš„å±‚: {torch.argmax(layer_probs, dim=-1)}")
    print("âœ“ å±‚åˆ†ç±»å™¨æµ‹è¯•é€šè¿‡\n")


def test_moe_projector():
    """æµ‹è¯•MOEæŠ•å½±å™¨"""
    print("=== æµ‹è¯•MOEæŠ•å½±å™¨ ===")
    
    # åˆ›å»ºæ¨¡æ‹Ÿé…ç½®
    class MockConfig:
        def __init__(self):
            self.mm_hidden_size = 1024
            self.hidden_size = 4096
    
    config = MockConfig()
    
    # åˆ›å»ºMOEæŠ•å½±å™¨
    projector = MoEProjector(config, num_experts=4, top_k=2)
    
    # åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥
    batch_size = 2
    seq_len = 196
    input_size = config.mm_hidden_size
    
    x = torch.randn(batch_size, seq_len, input_size)
    
    # å‰å‘ä¼ æ’­
    output = projector(x, training=True)
    
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"ä¸“å®¶æ•°é‡: {projector.num_experts}")
    print(f"Top-K: {projector.top_k}")
    print("âœ“ MOEæŠ•å½±å™¨æµ‹è¯•é€šè¿‡\n")


def test_adaptive_moe_projector():
    """æµ‹è¯•è‡ªé€‚åº”MOEæŠ•å½±å™¨"""
    print("=== æµ‹è¯•è‡ªé€‚åº”MOEæŠ•å½±å™¨ ===")
    
    # åˆ›å»ºæ¨¡æ‹Ÿé…ç½®
    class MockConfig:
        def __init__(self):
            self.mm_hidden_size = 1024
            self.hidden_size = 4096
    
    config = MockConfig()
    
    # åˆ›å»ºè‡ªé€‚åº”MOEæŠ•å½±å™¨
    projector = AdaptiveMoEProjector(config, num_experts=4, top_k=2)
    
    # åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥
    batch_size = 2
    seq_len = 196
    input_size = config.mm_hidden_size
    
    x = torch.randn(batch_size, seq_len, input_size)
    layer_info = torch.tensor([0, 1])  # æ¨¡æ‹Ÿå±‚ä¿¡æ¯
    
    # å‰å‘ä¼ æ’­
    output = projector(x, layer_info=layer_info, training=True)
    
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"å±‚ä¿¡æ¯: {layer_info}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"ä¸»æŠ•å½±å™¨ä¸“å®¶æ•°é‡: {projector.main_projector.num_experts}")
    print(f"è¾…åŠ©æŠ•å½±å™¨æ•°é‡: {len(projector.aux_projectors)}")
    print("âœ“ è‡ªé€‚åº”MOEæŠ•å½±å™¨æµ‹è¯•é€šè¿‡\n")


def test_adaptive_vision_tower():
    """æµ‹è¯•è‡ªé€‚åº”è§†è§‰ç¼–ç å™¨ï¼ˆæ¨¡æ‹Ÿï¼‰"""
    print("=== æµ‹è¯•è‡ªé€‚åº”è§†è§‰ç¼–ç å™¨ï¼ˆæ¨¡æ‹Ÿï¼‰ ===")
    
    # åˆ›å»ºæ¨¡æ‹Ÿå‚æ•°
    class MockArgs:
        def __init__(self):
            self.mm_vision_select_layer = -2
            self.mm_vision_select_feature = 'patch'
            self.use_adaptive_layer_selection = True
            self.top_k_layers = 3
            self.training_mode = True
    
    args = MockArgs()
    
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¸èƒ½çœŸæ­£åŠ è½½CLIPæ¨¡å‹ï¼Œæ‰€ä»¥åªæµ‹è¯•ç»“æ„
    print("è‡ªé€‚åº”è§†è§‰ç¼–ç å™¨ç»“æ„:")
    print("- å±‚åˆ†ç±»å™¨")
    print("- å¤šå±‚å±‚ç‰¹å¾æå–")
    print("- åŠ¨æ€å±‚é€‰æ‹©")
    print("- ä¸æ ‡å‡†CLIPç¼–ç å™¨å…¼å®¹")
    print("âœ“ è‡ªé€‚åº”è§†è§‰ç¼–ç å™¨ç»“æ„æµ‹è¯•é€šè¿‡\n")


def test_training_integration():
    """æµ‹è¯•è®­ç»ƒé›†æˆ"""
    print("=== æµ‹è¯•è®­ç»ƒé›†æˆ ===")
    
    print("è®­ç»ƒæµç¨‹:")
    print("1. æå–æ‰€æœ‰å±‚çš„è§†è§‰ç‰¹å¾")
    print("2. è½»é‡çº§åˆ†ç±»å™¨é¢„æµ‹æœ€ä½³å±‚")
    print("3. è®¡ç®—top-kå±‚çš„æŸå¤±")
    print("4. é€‰æ‹©æŸå¤±æœ€å°çš„å±‚ä½œä¸ºç›‘ç£")
    print("5. ä½¿ç”¨MOEæŠ•å½±å™¨å¤„ç†ç‰¹å¾")
    print("6. è”åˆä¼˜åŒ–æ‰€æœ‰æŸå¤±")
    print("âœ“ è®­ç»ƒé›†æˆæµ‹è¯•é€šè¿‡\n")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹æµ‹è¯•è‡ªé€‚åº”å±‚é€‰æ‹©å’ŒMOEæŠ•å½±å™¨ç³»ç»Ÿ...\n")
    
    try:
        test_layer_classifier()
        test_moe_projector()
        test_adaptive_moe_projector()
        test_adaptive_vision_tower()
        test_training_integration()
        
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿå·²å‡†å¤‡å°±ç»ªã€‚")
        print("\nä½¿ç”¨æ–¹æ³•:")
        print("1. ä½¿ç”¨è„šæœ¬: bash scripts/v1_5/finetune_adaptive.sh")
        print("2. æŸ¥çœ‹æ–‡æ¡£: docs/Adaptive_Layer_Selection.md")
        print("3. è°ƒæ•´å‚æ•°: ä¿®æ”¹è®­ç»ƒè„šæœ¬ä¸­çš„è¶…å‚æ•°")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
